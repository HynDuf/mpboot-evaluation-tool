# mpboot-evaluation-tool

## Folder structure
```
   ┌─ ...
   ├─ test_05_14_01_07          -->  Test run name
┌─ result                       -->  Result folders
│  ┌─ ...
│  ├─ test_05_14_01_07          -->  Test run name
├─ log                          -->  Log folders (*.log, *.treefile, *.mpboot...)
│     ┌─ ...
│     ├─ test_05_14_01_07       -->  Test run name
│  ┌─ scripts                   -->  Scripts folders (*.sh files save commands)
├─ debug
│  ┌─ mpboot-avx
├─ cmd                          -->  Binary of MPBoot versions
│     ┌─ *.phy
│  ┌─ example
│  │  ┌─ *.phy
│  ├─ treebase
├─ data                         -->  Dataset folders
├─ analyzer.py                  -->  Summarising results script
├─ run_test.py                  -->  Subscribe jobs to the system
├─ settings.py                  -->  Settings for running a test
├─ helper_func.py
mpboot-evaluation-tool
```
## How to use
- Change all the settings values in `settings.py`
    - `TEST_NAME`: Name of the test 
        - Ex: `test_05_14_01_07` (test running at 05:14 AM 01/07))
    - `PATH`: Full path to the `mpboot-evaluation-tool` folder (this repo's folder)
    - `DATA_PATH`: Full path to the dataset folder
    - `SUFFIXES`: Short capital unique names of all the MPBoot versions you want to test
        - ⚠️  Suffix must NOT contains any underscores (`_`)
    - `COMMANDS`: CMDs for running each MPBoot version
        - ⚠️  The order of the commands must be the SAME with `SUFFIXES`
        - ⚠️  Default cmds are using `qsub` for jobs scheduler (Please change accordingly)
    - `NUM_RUNS`: Number of times (seeds) to run for each testcase
    - `NUM_DATASET_FILES`: Number of files in the `DATA_PATH` folder
- `cd` to the `debug` folder (So that `*.o*` generated by `qsub` files won't pollute the repo's folder)
- Run `python ../run_test.py` (Or `python3 ../run_test.py` if your default one is Python 2)
    - This will subscribe all the jobs to the system (currently using `qsub`)
- Wait for all the jobs to finish.
- Run `python ../analyzer.py` (Or `python3 ../analyzer.py`).
    - A new folder named `TEST_NAME` will be generated in the `result` folder

## `result` folder 
- Metrics:
    - Standard Deviations from all-time best score (STDs):
        - STDs (1 dataset file):
            - $\sqrt{\dfrac{\sum\left({MP_{i}-MP_{all}}\right)^2}{NUM\_RUNS}}$
            - Where:
                - $MP_i$ is MP score of the i-th run (seed)
                - $MP_{all}$ is the globally best MP score found on all the runs among all the MPBoot versions
        - STDs of all dataset files: Sum of STDs of each file
    - Sum of computation time (secs)

- Content:
```
├── details                          --> Details of each MPBoot version (SUFFIXES)
│   ├── ACO
│   ├── IQP
│   ├── SPR6
│   ├── TBR5
│   └── ...
├── global.csv                       --> Global values like all-time best score...
├── summarise.csv                    --> Summarise results of all MPBoot versions
├── summarise_sorted_by_sum_stds.png --> Plot for `summarise.csv` sorted by STDs 
└── summarise_sorted_by_sum_time.png --> Plot for `summarise.csv` sorted by time
```

## Notes 
- The evaluation script contains some hardcode for MPBoot-ACO variants (Any `SUFFIXES` starts with `ACO` will have 2 additionally files (`aco.csv` and `aco_usages.png` for the usages of ACO types (RATCHET, IQP, RANDOM_NNI, NNI, SPR, TBR...))
